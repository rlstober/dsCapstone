---
title: "Data Science Capstone Project: Data Exploration"
author: "Robert Stober"
date: "March 26, 2015"
output: html_document
---

#Abstract

Around the world, people are spending an increasing amount of time on their mobile devices for email, social networking, banking and a whole range of other activities. But typing on mobile devices can be a serious pain. SwiftKey, our corporate partner in this capstone, builds a smart keyboard that makes it easier for people to type on their mobile devices. One cornerstone of their smart keyboard is predictive text models. 

The goal of this project is the understanding and building predictive text models like those used by SwiftKey. This project will analyze a large corpus of text documents to discover the structure in the data and how words are put together. It will cover cleaning and analyzing text data, then building and sampling from a predictive text model. Finally, we will build a predictive text shiny app to demonstrate the product, and a slide deck to pitch it.

This paper focuses on the initial exploration and analysis of the data.


# Data Provenance

This is the training data to get you started that will be the basis for most of the capstone. You must download the data from the Coursera site and not from external websites to start.

Capstone Dataset
https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

This exercise uses the files named LOCALE.blogs.txt where LOCALE is the each of the four locales en_US, de_DE, ru_RU and fi_FI. The data is from a corpus called HC Corpora (www.corpora.heliohost.org). See the readme file at:
http://www.corpora.heliohost.org/aboutcorpus.html for details on the corpora available.


# Exploratory Analysis

The following libraries were used

```{r eval=FALSE}
library(stringi)
library(ggplot2)
library(tm)
library(RWeka)
library(slam)
```

## Data Files

Files were downloaded and unzipped. 

```{r eval=FALSE}
## Download process
#destFile <- "./data/Coursera-SwiftKey.zip"
#sourceFile <- "http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
#download.file(sourceFile, destFile)
#unzip(destFile)
```

The English locales files were read. We took a look at file sizes, number of words and characters per lines to get an overall sense of the datasets. The data shows sight differences due to sources.

Some cleanup was performed at this point.

  1. Text was converted to lower
  2. Text was converted to unicode
  3. New line characters were removed.
  4. End of sentence pronunciation ws removed.

The Blog is the largest dataset. It does so even though it has the fewest lines, by also having the max characters per line yet. Conversely, twitter the smallest dataset has the most lines but fewest characters per line. Twitter likely has the fewest characters per line due to its 140 character limit. The length of a news line is slightly less than a blog, which may reflect a need of brevity in reporting stories, not present in blogs.


```{r echo = FALSE}
load("./data/fileDF.RData");print(fileDF)
```


## String Statistics

We also looked at string statistics. In this case the proportion of whitespace characters to word characters is roughly  the same, at 26%, 28% and 24% for blog, twitter and news,

```{r echo = FALSE}
load("./data/statsDF.RData");print(statsDF)
```

## Word Counts

IQR of Word Counts. The intra-quartile range or IQR is shown below. It further demonstrates the differences in the three datasets. For example the means are different across all three and while the median is close to the mean for twitter and news, they are skewed for blogs.

```{r echo = FALSE}
load("./data/wordsDF.RData");print(wordsDF)
```


##Word Frequency Comparison

The graphs below further illustrate the different profiles of the data sets.

![Blog Words Word Frequencies](C:\Users\bstober\Documents\GitHub\dsCapstone\data\blogWords.png)

![Blog Words Word Frequencies](C:\Users\bstober\Documents\GitHub\dsCapstone\data\twitterWords.png)

![Blog Words Word Frequencies](C:\Users\bstober\Documents\GitHub\dsCapstone\data\newsWords.png)


#N-Gram Analysis

In this step we create a Term Document Matrix (TDM) from the three documents. We built a corpus based on 10,000 line samples from each text source. Preprocessing steps in creation of the TDM included:

  1. Remove extra whitespace
  2. Remove profanity (as per George Carlin)
  3. Remove numbers

Below is a summary of word counts for 1-4 N-grams.

```{r echo = FALSE}
load("./data/ngramMat.RData");print(ngramMat)
```

##N-Gram Frequency Comparison

The following histograms show the top 20 words in the n-grams.

![Blog Words Word Frequencies](C:\Users\bstober\Documents\GitHub\dsCapstone\data\uniTDMfreq20.png)

![Blog Words Word Frequencies](C:\Users\bstober\Documents\GitHub\dsCapstone\data\biTDMfreq20.png)

![Blog Words Word Frequencies](C:\Users\bstober\Documents\GitHub\dsCapstone\data\triTDMfreq20.png)

![Blog Words Word Frequencies](C:\Users\bstober\Documents\GitHub\dsCapstone\data\quadTDMfreq20.png)



#Conclusions

The source data comes from three distinct data sets, Based on an exploration of that data it seems an ideal predictor would utilize different models depending on the source. As that i beyond the scope of this project, our predictor will not make any assumptions about the source of the data.

Our predictive model will be based off the n-grams created. This would utilizing a lookup table based on frequency. Markov chains would be used to efficiently store the n-grams. Emphasis will be running fast predictions using limited memory. Back off models will need to be employed for unobserved n-grams.

Data will be split into Training and Testing sets to help determine better predictive models. Some measure of accuracy will be employed. We may also look into Perplexity as a meause fo model quality.

The shiny app will mimic the Swiftkey application. Based on an input of 1-3 words, the application will suggest three words for the user to choose from. 

