---
title: "Data Science Capstone Project: Data Exploration"
author: "Robert Stober"
date: "March 26, 2015"
output: html_document
---

#Abstract

Around the world, people are spending an increasing amount of time on their mobile devices for email, social networking, banking and a whole range of other activities. But typing on mobile devices can be a serious pain. SwiftKey, our corporate partner in this capstone, builds a smart keyboard that makes it easier for people to type on their mobile devices. One cornerstone of their smart keyboard is predictive text models. 

The goal of this project is the understanding and building predictive text models like those used by SwiftKey.This project will analyze a large corpus of text documents to discover the structure in the data and how words are put together. It will cover cleaning and analyzing text data, then building and sampling from a predictive text model. Finally, we will build a predictive text shiny app to demonstrate the product, and a slide deck to pitch it.

This paper focuses on the initial exploration and analysis of the data.


# Data Provenance

This is the training data to get you started that will be the basis for most of the capstone. You must download the data from the Coursera site and not from external websites to start.

Capstone Dataset
https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

This exercise uses the files named LOCALE.blogs.txt where LOCALE is the each of the four locales en_US, de_DE, ru_RU and fi_FI. The data is from a corpus called HC Corpora (www.corpora.heliohost.org). See the readme file at:
http://www.corpora.heliohost.org/aboutcorpus.html for details on the corpora available.


# Exploratory Analysis

The followinf libraries were used

```{r}
library(stringi)
library(ggplot2)

```


Files were downloaded and unzipped. The english locales files were read. We took a look at file sizes, number of words and characters per lines toget an overall sense of the datasets. The data shows sight differences due to sources.

The Blog is the largest dataset. It does so even though it has the fewst lines, by also having the max charaxcters per line yet. Converesly, twitter the smallest datset has the most lines but fewest characters per line. Twitter likely has the fewest characters per line due to it's 140 charcater limit. The length of a news line is slightly less than a blog, which may reflect a need of brevity in reporitgn stories, not present in blogs.

```{r}
load("./data/fileDF.RData");print(fileDF)
```


IQR of Word Counts. The intra-quatrile range or IQR is shown below. It further demonstartes the differences in the three datasets. For example the mean 

```{r}
load("./data/wordsDF.RData");print(wordsDF)
```


We also looked at string Statistics. In this case the proportion of whitespace characters to word characters is roungly the same, at 26%, 28% and 24% for blog, twitter and news,


```{r}
load("./data/statsDF.RData");print(statsDF)
```


Pictures

![Blog Words Word G=frequencies]("//localhost://Users/bstober/Documents/GitHub/dsCapstone/data/blogWords.png")




#source("./data/blogWords.png")

#library(png)
#blogWords<-png(file="C:\Users\bstober\Documents\GitHub\dsCapstone\data\blogWords.png")

qplot(png(file="./data/blogWords.png"))
plot





#Preprocess and Train

In this step we preprocess the data using PCA-principal component Analysis. The threshold parameter is set at 95% meaning that number of components chosen will explain 95% of the variance. The overall summary of the model and rank of variable importance is presented.



#Results

The predictions were correct in 19 out of 20 or 95% of the test cases. This 5% error is higher than the estimated 2.3%, but is not unreasonable considering the small number of cases being tested. It may also effect bias or over-fitting in the model.


#Conclusions

based on this study it appears that predicting the type of exercise performed, where type classifies if the exercise is done correctly, based on features collected via personal fitness devices is achievable with a high degree of accuracy. 